{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QuestionAnswering.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPpY2mkU5B+FqNCDqEfkw9J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"o2MST6LiTNRN"},"outputs":[],"source":["!git clone https://github.com/huggingface/transformers\n","%cd transformers\n","!pip install ."]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","from transformers import BertForQuestionAnswering\n","import torch\n","# Initialize tokenizer for corpus of bert-large-uncased\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","\n","# Initialize model BertForQuestionAnswering for bert-large-uncased\n","model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"],"metadata":{"id":"VjpmkPUbTUUM","executionInfo":{"status":"ok","timestamp":1646237002652,"user_tz":-420,"elapsed":10901,"user":{"displayName":"Hoang Long Dao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16503700992091421007"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def answer_question(question, answer_text):\n","    '''\n","    Takes a `question` string and an `answer_text` string (which contains the\n","    answer), and identifies the words within the `answer_text` that are the\n","    answer. Prints them out.\n","    '''\n","    # ======== Tokenize ========\n","    # Apply the tokenizer to the input text, treating them as a text-pair.\n","    input_ids = tokenizer.encode(question, answer_text)\n","\n","    # Report how long the input sequence is.\n","    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n","\n","    # ======== Set Segment IDs ========\n","    # Search the input_ids for the first instance of the `[SEP]` token.\n","    sep_index = input_ids.index(tokenizer.sep_token_id)\n","\n","    # The number of segment A tokens includes the [SEP] token istelf.\n","    num_seg_a = sep_index + 1\n","\n","    # The remainder are segment B.\n","    num_seg_b = len(input_ids) - num_seg_a\n","\n","    # Construct the list of 0s and 1s.\n","    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n","\n","    # There should be a segment_id for every input token.\n","    assert len(segment_ids) == len(input_ids)\n","\n","    # ======== Evaluate ========\n","    # Run our example question through the model.\n","    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n","                                    token_type_ids=torch.tensor([segment_ids]), return_dict=False) # The segment IDs to differentiate question from answer_text\n","\n","    # ======== Reconstruct Answer ========\n","    # Find the tokens with the highest `start` and `end` scores.\n","    answer_start = torch.argmax(start_scores)\n","    answer_end = torch.argmax(end_scores)\n","\n","    # Get the string versions of the input tokens.\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","    # Start with the first token.\n","    answer = tokens[answer_start]\n","\n","    # Select the remaining answer tokens and join them with whitespace.\n","    for i in range(answer_start + 1, answer_end + 1):\n","        \n","        # If it's a subword token, then recombine it with the previous token.\n","        if tokens[i][0:2] == '##':\n","            answer += tokens[i][2:]\n","        \n","        # Otherwise, add a space then the token.\n","        else:\n","            answer += ' ' + tokens[i]\n","\n","    print('Answer: \"' + answer + '\"')"],"metadata":{"id":"u1h8j6DqUy1i","executionInfo":{"status":"ok","timestamp":1646237114211,"user_tz":-420,"elapsed":471,"user":{"displayName":"Hoang Long Dao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16503700992091421007"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import textwrap\n","\n","# Wrap text to 80 characters.\n","wrapper = textwrap.TextWrapper(width=80) \n","\n","bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n","\n","print(wrapper.fill(bert_abstract))"],"metadata":{"id":"og2A70j9TYf1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"What does the 'B' in BERT stand for?\"\n","\n","answer_question(question, bert_abstract)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rVdMISWW_uT","executionInfo":{"status":"ok","timestamp":1646237122614,"user_tz":-420,"elapsed":5326,"user":{"displayName":"Hoang Long Dao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16503700992091421007"}},"outputId":"1a3ecb72-f7bb-4448-ac10-cbac1bbbdedf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Query has 258 tokens.\n","\n","Answer: \"bidirectional encoder representations from transformers\"\n"]}]}]}